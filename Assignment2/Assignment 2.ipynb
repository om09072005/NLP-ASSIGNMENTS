{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec55f0d",
   "metadata": {},
   "source": [
    "#Perform bag-of-words approach (count occurrence, normalized count occurrence)\n",
    "#TF-IDF on data.\n",
    "#Create embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e3212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
      "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.4 MB 3.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.3/24.4 MB 3.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.1/24.4 MB 3.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 3.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 3.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 3.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.2/24.4 MB 2.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 4.7/24.4 MB 2.7 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 5.5/24.4 MB 2.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 3.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 7.3/24.4 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 8.1/24.4 MB 3.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 8.9/24.4 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.7/24.4 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.5/24.4 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.3/24.4 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.1/24.4 MB 3.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 14.7/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.5/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.5/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.9/24.4 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.7/24.4 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.2/24.4 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67718b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0048c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data (Replace this with your dataset)\n",
    "documents = [\n",
    "    \"AI is transforming the future.\",\n",
    "    \"Machine learning is a part of AI.\",\n",
    "    \"Natural language processing is useful for AI applications.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85c42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(documents, columns=[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a600166",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(df[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words (Raw Count):\n",
      "   ai  applications  for  future  is  language  learning  machine  natural  \\\n",
      "0   1             0    0       1   1         0         0        0        0   \n",
      "1   1             0    0       0   1         0         1        1        0   \n",
      "2   1             1    1       0   1         1         0        0        1   \n",
      "\n",
      "   of  part  processing  the  transforming  useful  \n",
      "0   0     0           0    1             1       0  \n",
      "1   1     1           0    0             0       0  \n",
      "2   0     0           1    0             0       1  \n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"Bag-of-Words (Raw Count):\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "192d2e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag-of-Words (Normalized Count):\n",
      "         ai  applications    for  future        is  language  learning  \\\n",
      "0  0.200000         0.000  0.000     0.2  0.200000     0.000  0.000000   \n",
      "1  0.166667         0.000  0.000     0.0  0.166667     0.000  0.166667   \n",
      "2  0.125000         0.125  0.125     0.0  0.125000     0.125  0.000000   \n",
      "\n",
      "    machine  natural        of      part  processing  the  transforming  \\\n",
      "0  0.000000    0.000  0.000000  0.000000       0.000  0.2           0.2   \n",
      "1  0.166667    0.000  0.166667  0.166667       0.000  0.0           0.0   \n",
      "2  0.000000    0.125  0.000000  0.000000       0.125  0.0           0.0   \n",
      "\n",
      "   useful  \n",
      "0   0.000  \n",
      "1   0.000  \n",
      "2   0.125  \n"
     ]
    }
   ],
   "source": [
    "normalized_bow = normalize(bow_matrix, norm='l1', axis=1)  # L1 Normalization\n",
    "# Convert sparse matrix to dense array to solve shape mismatch.\n",
    "normalized_bow_dense = normalized_bow.toarray()\n",
    "normalized_bow_df = pd.DataFrame(normalized_bow_dense, columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nBag-of-Words (Normalized Count):\")\n",
    "print(normalized_bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer() # Initialize the TfidfVectorizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"Text\"]) # Fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3fda0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "         ai  applications       for   future        is  language  learning  \\\n",
      "0  0.307144      0.000000  0.000000  0.52004  0.307144  0.000000  0.000000   \n",
      "1  0.272499      0.000000  0.000000  0.00000  0.272499  0.000000  0.461381   \n",
      "2  0.228215      0.386401  0.386401  0.00000  0.228215  0.386401  0.000000   \n",
      "\n",
      "    machine   natural        of      part  processing      the  transforming  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000    0.000000  0.52004       0.52004   \n",
      "1  0.461381  0.000000  0.461381  0.461381    0.000000  0.00000       0.00000   \n",
      "2  0.000000  0.386401  0.000000  0.000000    0.386401  0.00000       0.00000   \n",
      "\n",
      "     useful  \n",
      "0  0.000000  \n",
      "1  0.000000  \n",
      "2  0.386401  \n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "004c6d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the 'punkt_tab' resource:\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a2a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the sentences for Word2Vec\n",
    "tokenized_text = [word_tokenize(doc.lower()) for doc in df[\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283dea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec vector for 'ai':\n",
      "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
     ]
    }
   ],
   "source": [
    "# Get vector for a word\n",
    "word = \"ai\"\n",
    "if word in word2vec_model.wv:\n",
    "    print(f\"\\nWord2Vec vector for '{word}':\")\n",
    "    print(word2vec_model.wv[word])\n",
    "else:\n",
    "    print(f\"\\nWord '{word}' not found in Word2Vec vocabulary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
